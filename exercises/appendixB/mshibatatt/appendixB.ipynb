{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. *Generalization* \n",
    "Install Keras. Go to the Keras MNIST example. Perform a classification task. Note how many epochs the training takes, and in testing, how well it generalizes. Perform the classification on a smaller training set, how does learning rate change, how does generalization change. Vary other elements: try a different optimizer than adam, try a different learning rate, try a different (deeper) architecture, try wider hidden layers. Does it learn faster? Does it generalize better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/examples/vision/mnist_convnet/\n",
    "num_classes = 18\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "print(\"x_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-19 16:39:00.165471: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-19 16:39:00.165607: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-19 16:39:00.165666: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ee1eba175434): /proc/driver/nvidia/version does not exist\n",
      "2022-04-19 16:39:00.166342: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 18)                28818     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,634\n",
      "Trainable params: 47,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-19 16:40:06.917286: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 169344000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 84s 195ms/step - loss: 0.4033 - accuracy: 0.8752 - val_loss: 0.0942 - val_accuracy: 0.9750\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 79s 188ms/step - loss: 0.1285 - accuracy: 0.9616 - val_loss: 0.0712 - val_accuracy: 0.9818\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 86s 205ms/step - loss: 0.0916 - accuracy: 0.9721 - val_loss: 0.0500 - val_accuracy: 0.9875\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 84s 200ms/step - loss: 0.0751 - accuracy: 0.9770 - val_loss: 0.0414 - val_accuracy: 0.9887\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 80s 190ms/step - loss: 0.0687 - accuracy: 0.9787 - val_loss: 0.0414 - val_accuracy: 0.9893\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 98s 233ms/step - loss: 0.0588 - accuracy: 0.9816 - val_loss: 0.0370 - val_accuracy: 0.9903\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 85s 202ms/step - loss: 0.0556 - accuracy: 0.9821 - val_loss: 0.0375 - val_accuracy: 0.9895\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 79s 187ms/step - loss: 0.0526 - accuracy: 0.9832 - val_loss: 0.0390 - val_accuracy: 0.9883\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 90s 213ms/step - loss: 0.0475 - accuracy: 0.9849 - val_loss: 0.0345 - val_accuracy: 0.9907\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 90s 212ms/step - loss: 0.0460 - accuracy: 0.9857 - val_loss: 0.0320 - val_accuracy: 0.9918\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 86s 204ms/step - loss: 0.0439 - accuracy: 0.9857 - val_loss: 0.0301 - val_accuracy: 0.9918\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 79s 187ms/step - loss: 0.0406 - accuracy: 0.9866 - val_loss: 0.0319 - val_accuracy: 0.9918\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 79s 188ms/step - loss: 0.0387 - accuracy: 0.9876 - val_loss: 0.0293 - val_accuracy: 0.9917\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 79s 186ms/step - loss: 0.0381 - accuracy: 0.9876 - val_loss: 0.0294 - val_accuracy: 0.9915\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 79s 187ms/step - loss: 0.0353 - accuracy: 0.9887 - val_loss: 0.0307 - val_accuracy: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7c8721ef9130>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.02899838238954544\n",
      "Test accuracy: 0.989799976348877\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. *Overfitting* \n",
    "Use Keras again, but this time on ImageNet. Now try different over-fitting solutions. Does the training speed change? Does generalization change? Now try the hold-out validation set. Do training and generalization change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. *Confidence* \n",
    "How many runs did you do in the previous exercises, just a single run to see how long training took and how well generalization worked? Try to run it again. Do you get the same results? How large is the difference? Can you change the random seeds of Keras or TensorFlow? Can you calculate the confidence interval, how much does the confidence improve when you do 10 randomized runs? How about 100 runs? Make graphs with error bars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. *GPU*\n",
    "It might be that you have access to a GPU machine that is capable of running PyTorch or TensorFlow in parallel to speed up the training. Install the GPU version and check that it recognizes the GPU and is indeed using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. *Parallelism*\n",
    "It might be that you have access to a multicore CPU machine. When you are running multiple runs in order to improve confiedence, then an easy way to speed up your experiment is to spawn multiple jobs at the shell, assigning the output to different log files, and write a script to combine results and draw graphs. Write the scripts necessary to achieve this, test them, and do a large-confidence experiment."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
