{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter2\n",
    "#### E1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(gamma, alpha, epsilon, showReward=True, showQ=True):\n",
    "    # Q learning for OpenAI Gym Taxi environment\n",
    "    import gym\n",
    "    import numpy as np\n",
    "    import random\n",
    "\n",
    "    # Environment Setup\n",
    "    env = gym.make(\"Taxi-v3\")\n",
    "    env.reset()\n",
    "\n",
    "    # Q[state, action] table implementation\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    for episode in range(1000):\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()  # Explore state space\n",
    "            else:\n",
    "                action = np.argmax(Q[state])  # Exploit learned values\n",
    "            next_state, reward, done, info = env.step(action)  # invoke Gym\n",
    "            next_max = np.max(Q[next_state])\n",
    "            old_value = Q[state, action]\n",
    "\n",
    "            new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "\n",
    "            Q[state, action] = new_value\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            if showReward:\n",
    "                print(f\"Episode {episode} Total Reward: {total_reward}\")\n",
    "    if showQ:\n",
    "        print(Q)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- print a live policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Total Reward: -569\n",
      "Episode 100 Total Reward: -245\n",
      "Episode 200 Total Reward: -55\n",
      "Episode 300 Total Reward: -59\n",
      "Episode 400 Total Reward: 11\n",
      "Episode 500 Total Reward: -15\n",
      "Episode 600 Total Reward: 14\n",
      "Episode 700 Total Reward: 10\n",
      "Episode 800 Total Reward: -1\n",
      "Episode 900 Total Reward: -13\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [-3.19670474 -3.11377344 -3.32553749 -3.19420794 -1.64506284 -5.53472077]\n",
      " [-1.61313746 -2.13501361 -2.05284028 -1.42595097  3.1919773  -5.23663549]\n",
      " ...\n",
      " [-1.33321462 -1.36678377 -1.48131366 -1.50702631 -3.632      -3.66272   ]\n",
      " [-2.50206596 -2.4659207  -2.50206596 -2.12406715 -3.90706105 -3.77379377]\n",
      " [-0.392      -0.392      -0.392      10.17655029 -2.         -2.        ]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.8 # discount factor{}\n",
    "alpha = 0.2  # learning rate\n",
    "epsilon = 0.1  # epsilon greedy\n",
    "\n",
    "Q_learning(gamma, alpha, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon:0.1\n",
      "Episode 0 Total Reward: -578\n",
      "Episode 100 Total Reward: 10\n",
      "Episode 200 Total Reward: -51\n",
      "Episode 300 Total Reward: -78\n",
      "Episode 400 Total Reward: 10\n",
      "Episode 500 Total Reward: -15\n",
      "Episode 600 Total Reward: -11\n",
      "Episode 700 Total Reward: -1\n",
      "Episode 800 Total Reward: 10\n",
      "Episode 900 Total Reward: -2\n",
      "epsilon:0.01\n",
      "Episode 0 Total Reward: -533\n",
      "Episode 100 Total Reward: -209\n",
      "Episode 200 Total Reward: -52\n",
      "Episode 300 Total Reward: -71\n",
      "Episode 400 Total Reward: -94\n",
      "Episode 500 Total Reward: 0\n",
      "Episode 600 Total Reward: 10\n",
      "Episode 700 Total Reward: 9\n",
      "Episode 800 Total Reward: -18\n",
      "Episode 900 Total Reward: 3\n",
      "epsilon:0.001\n",
      "Episode 0 Total Reward: -560\n",
      "Episode 100 Total Reward: -209\n",
      "Episode 200 Total Reward: -66\n",
      "Episode 300 Total Reward: -68\n",
      "Episode 400 Total Reward: -56\n",
      "Episode 500 Total Reward: -4\n",
      "Episode 600 Total Reward: -16\n",
      "Episode 700 Total Reward: 8\n",
      "Episode 800 Total Reward: 6\n",
      "Episode 900 Total Reward: 9\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.8 # discount factor\n",
    "alpha = 0.2  # learning rate\n",
    "epsilons = [0.1, 0.01, 0.001]  # epsilon greedy\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    print(f\"epsilon:{epsilon}\")\n",
    "    Q_learning(gamma, alpha, epsilon, True, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:0.5\n",
      "Episode 0 Total Reward: -596\n",
      "Episode 100 Total Reward: -142\n",
      "Episode 200 Total Reward: 10\n",
      "Episode 300 Total Reward: 11\n",
      "Episode 400 Total Reward: 3\n",
      "Episode 500 Total Reward: 12\n",
      "Episode 600 Total Reward: 4\n",
      "Episode 700 Total Reward: 5\n",
      "Episode 800 Total Reward: 10\n",
      "Episode 900 Total Reward: 10\n",
      "alpha:0.2\n",
      "Episode 0 Total Reward: -542\n",
      "Episode 100 Total Reward: -452\n",
      "Episode 200 Total Reward: -112\n",
      "Episode 300 Total Reward: -200\n",
      "Episode 400 Total Reward: 12\n",
      "Episode 500 Total Reward: -99\n",
      "Episode 600 Total Reward: -6\n",
      "Episode 700 Total Reward: 11\n",
      "Episode 800 Total Reward: 10\n",
      "Episode 900 Total Reward: 10\n",
      "alpha:0.1\n",
      "Episode 0 Total Reward: -497\n",
      "Episode 100 Total Reward: -151\n",
      "Episode 200 Total Reward: -290\n",
      "Episode 300 Total Reward: -169\n",
      "Episode 400 Total Reward: -67\n",
      "Episode 500 Total Reward: -151\n",
      "Episode 600 Total Reward: -45\n",
      "Episode 700 Total Reward: -84\n",
      "Episode 800 Total Reward: -20\n",
      "Episode 900 Total Reward: -116\n",
      "alpha:0.01\n",
      "Episode 0 Total Reward: -524\n",
      "Episode 100 Total Reward: -245\n",
      "Episode 200 Total Reward: -290\n",
      "Episode 300 Total Reward: -162\n",
      "Episode 400 Total Reward: -271\n",
      "Episode 500 Total Reward: -308\n",
      "Episode 600 Total Reward: -272\n",
      "Episode 700 Total Reward: -254\n",
      "Episode 800 Total Reward: -201\n",
      "Episode 900 Total Reward: -299\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.8 # discount factor\n",
    "alphas = [0.5, 0.2 , 0.1, 0.01]  # learning rate\n",
    "epsilons = 0.1 # epsilon greedy\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(f\"alpha:{alpha}\")\n",
    "    Q_learning(gamma, alpha, epsilon, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def SARSA(gamma, alpha, epsilon, showReward=True):\n",
    "    import gym\n",
    "    import numpy as np\n",
    "    import random\n",
    "    #Environment Setup\n",
    "    env = gym.make(\"Taxi-v3\")\n",
    "    env.reset()\n",
    "    # Q[state ,action] table implementation\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n]) \n",
    "\n",
    "\n",
    "    for episode in range(1000):\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        current_state = env.reset()\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            current_action = env.action_space.sample() # Explore state space\n",
    "        else:\n",
    "            current_action = np.argmax(Q[current_state]) # Exploit learned values\n",
    "        while not done:\n",
    "            next_state, reward, done, info = env.step(current_action)\n",
    "            # invoke Gym\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                next_action = env.action_space.sample() # Explore state space\n",
    "            else:\n",
    "                next_action = np.argmax(Q[next_state]) # Exploit learned values\n",
    "            sarsa_value = Q[next_state ,next_action] \n",
    "            old_value = Q[current_state ,current_action]\n",
    "            new_value = old_value + alpha * (reward + gamma * sarsa_value - old_value)\n",
    "            \n",
    "            Q[current_state ,current_action] = new_value\n",
    "            total_reward += reward \n",
    "            current_state = next_state \n",
    "            current_action = next_action\n",
    "\n",
    "        if episode % 100 == 0: \n",
    "            if showReward:\n",
    "                print(\"Episode {} Total Reward: {}\".format(episode ,total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Total Reward: -695\n",
      "Episode 100 Total Reward: -233\n",
      "Episode 200 Total Reward: -168\n",
      "Episode 300 Total Reward: -136\n",
      "Episode 400 Total Reward: -91\n",
      "Episode 500 Total Reward: 12\n",
      "Episode 600 Total Reward: -67\n",
      "Episode 700 Total Reward: -4\n",
      "Episode 800 Total Reward: -236\n",
      "Episode 900 Total Reward: -236\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.7 # discount factor\n",
    "alpha = 0.2 # learning rate\n",
    "epsilon = 0.1 # epsilon greedy\n",
    "\n",
    "SARSA(gamma, alpha, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon:0.1\n",
      "Episode 0 Total Reward: -695\n",
      "Episode 100 Total Reward: -263\n",
      "Episode 200 Total Reward: -134\n",
      "Episode 300 Total Reward: -181\n",
      "Episode 400 Total Reward: -200\n",
      "Episode 500 Total Reward: -2\n",
      "Episode 600 Total Reward: -134\n",
      "Episode 700 Total Reward: -28\n",
      "Episode 800 Total Reward: 11\n",
      "Episode 900 Total Reward: -54\n",
      "epsilon:0.01\n",
      "Episode 0 Total Reward: -758\n",
      "Episode 100 Total Reward: -209\n",
      "Episode 200 Total Reward: -14\n",
      "Episode 300 Total Reward: -39\n",
      "Episode 400 Total Reward: -2\n",
      "Episode 500 Total Reward: -13\n",
      "Episode 600 Total Reward: 6\n",
      "Episode 700 Total Reward: -44\n",
      "Episode 800 Total Reward: 5\n",
      "Episode 900 Total Reward: 0\n",
      "epsilon:0.001\n",
      "Episode 0 Total Reward: -740\n",
      "Episode 100 Total Reward: -200\n",
      "Episode 200 Total Reward: -103\n",
      "Episode 300 Total Reward: -27\n",
      "Episode 400 Total Reward: 1\n",
      "Episode 500 Total Reward: 9\n",
      "Episode 600 Total Reward: 4\n",
      "Episode 700 Total Reward: 11\n",
      "Episode 800 Total Reward: 13\n",
      "Episode 900 Total Reward: 0\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.8 # discount factor\n",
    "alpha = 0.2  # learning rate\n",
    "epsilons = [0.1, 0.01, 0.001]  # epsilon greedy\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    print(f\"epsilon:{epsilon}\")\n",
    "    SARSA(gamma, alpha, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:0.5\n",
      "Episode 0 Total Reward: -740\n",
      "Episode 100 Total Reward: -200\n",
      "Episode 200 Total Reward: -99\n",
      "Episode 300 Total Reward: -67\n",
      "Episode 400 Total Reward: 2\n",
      "Episode 500 Total Reward: 7\n",
      "Episode 600 Total Reward: 9\n",
      "Episode 700 Total Reward: 6\n",
      "Episode 800 Total Reward: 7\n",
      "Episode 900 Total Reward: 8\n",
      "alpha:0.2\n",
      "Episode 0 Total Reward: -776\n",
      "Episode 100 Total Reward: -200\n",
      "Episode 200 Total Reward: -84\n",
      "Episode 300 Total Reward: -344\n",
      "Episode 400 Total Reward: -91\n",
      "Episode 500 Total Reward: -11\n",
      "Episode 600 Total Reward: 5\n",
      "Episode 700 Total Reward: 8\n",
      "Episode 800 Total Reward: 8\n",
      "Episode 900 Total Reward: 5\n",
      "alpha:0.1\n",
      "Episode 0 Total Reward: -740\n",
      "Episode 100 Total Reward: -200\n",
      "Episode 200 Total Reward: -380\n",
      "Episode 300 Total Reward: -27\n",
      "Episode 400 Total Reward: -67\n",
      "Episode 500 Total Reward: -146\n",
      "Episode 600 Total Reward: -22\n",
      "Episode 700 Total Reward: 11\n",
      "Episode 800 Total Reward: 12\n",
      "Episode 900 Total Reward: -16\n",
      "alpha:0.01\n",
      "Episode 0 Total Reward: -731\n",
      "Episode 100 Total Reward: -299\n",
      "Episode 200 Total Reward: -272\n",
      "Episode 300 Total Reward: -236\n",
      "Episode 400 Total Reward: -236\n",
      "Episode 500 Total Reward: -220\n",
      "Episode 600 Total Reward: -36\n",
      "Episode 700 Total Reward: -236\n",
      "Episode 800 Total Reward: -200\n",
      "Episode 900 Total Reward: -209\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.8 # discount factor\n",
    "alphas = [0.5, 0.2 , 0.1, 0.01]  # learning rate\n",
    "epsilons = 0.1 # epsilon greedy\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(f\"alpha:{alpha}\")\n",
    "    SARSA(gamma, alpha, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Episode finished after 26 time steps / mean 0.000000\n",
      "1 Episode finished after 14 time steps / mean 0.260000\n",
      "2 Episode finished after 30 time steps / mean 0.400000\n",
      "3 Episode finished after 14 time steps / mean 0.700000\n",
      "4 Episode finished after 21 time steps / mean 0.840000\n",
      "5 Episode finished after 10 time steps / mean 1.050000\n",
      "6 Episode finished after 38 time steps / mean 1.150000\n",
      "7 Episode finished after 14 time steps / mean 1.530000\n",
      "8 Episode finished after 11 time steps / mean 1.670000\n",
      "9 Episode finished after 16 time steps / mean 1.780000\n",
      "10 Episode finished after 20 time steps / mean 1.940000\n",
      "11 Episode finished after 16 time steps / mean 2.140000\n",
      "12 Episode finished after 21 time steps / mean 2.300000\n",
      "13 Episode finished after 20 time steps / mean 2.510000\n",
      "14 Episode finished after 20 time steps / mean 2.710000\n",
      "15 Episode finished after 30 time steps / mean 2.910000\n",
      "16 Episode finished after 21 time steps / mean 3.210000\n",
      "17 Episode finished after 27 time steps / mean 3.420000\n",
      "18 Episode finished after 11 time steps / mean 3.690000\n",
      "19 Episode finished after 23 time steps / mean 3.800000\n",
      "20 Episode finished after 11 time steps / mean 4.030000\n",
      "21 Episode finished after 23 time steps / mean 4.140000\n",
      "22 Episode finished after 28 time steps / mean 4.370000\n",
      "23 Episode finished after 15 time steps / mean 4.650000\n",
      "24 Episode finished after 14 time steps / mean 4.800000\n",
      "25 Episode finished after 64 time steps / mean 4.940000\n",
      "26 Episode finished after 16 time steps / mean 5.580000\n",
      "27 Episode finished after 11 time steps / mean 5.740000\n",
      "28 Episode finished after 12 time steps / mean 5.850000\n",
      "29 Episode finished after 14 time steps / mean 5.970000\n",
      "30 Episode finished after 12 time steps / mean 6.110000\n",
      "31 Episode finished after 13 time steps / mean 6.230000\n",
      "32 Episode finished after 10 time steps / mean 6.360000\n",
      "33 Episode finished after 13 time steps / mean 6.460000\n",
      "34 Episode finished after 30 time steps / mean 6.590000\n",
      "35 Episode finished after 28 time steps / mean 6.890000\n",
      "36 Episode finished after 43 time steps / mean 7.170000\n",
      "37 Episode finished after 16 time steps / mean 7.600000\n",
      "38 Episode finished after 13 time steps / mean 7.760000\n",
      "39 Episode finished after 28 time steps / mean 7.890000\n",
      "40 Episode finished after 33 time steps / mean 8.170000\n",
      "41 Episode finished after 30 time steps / mean 8.500000\n",
      "42 Episode finished after 19 time steps / mean 8.800000\n",
      "43 Episode finished after 23 time steps / mean 8.990000\n",
      "44 Episode finished after 10 time steps / mean 9.220000\n",
      "45 Episode finished after 42 time steps / mean 9.320000\n",
      "46 Episode finished after 22 time steps / mean 9.740000\n",
      "47 Episode finished after 13 time steps / mean 9.960000\n",
      "48 Episode finished after 25 time steps / mean 10.090000\n",
      "49 Episode finished after 13 time steps / mean 10.340000\n",
      "50 Episode finished after 28 time steps / mean 10.470000\n",
      "51 Episode finished after 27 time steps / mean 10.750000\n",
      "52 Episode finished after 21 time steps / mean 11.020000\n",
      "53 Episode finished after 16 time steps / mean 11.230000\n",
      "54 Episode finished after 46 time steps / mean 11.390000\n",
      "55 Episode finished after 27 time steps / mean 11.850000\n",
      "56 Episode finished after 35 time steps / mean 12.120000\n",
      "57 Episode finished after 17 time steps / mean 12.470000\n",
      "58 Episode finished after 26 time steps / mean 12.640000\n",
      "59 Episode finished after 25 time steps / mean 12.900000\n",
      "60 Episode finished after 14 time steps / mean 13.150000\n",
      "61 Episode finished after 29 time steps / mean 13.290000\n",
      "62 Episode finished after 28 time steps / mean 13.580000\n",
      "63 Episode finished after 17 time steps / mean 13.860000\n",
      "64 Episode finished after 22 time steps / mean 14.030000\n",
      "65 Episode finished after 48 time steps / mean 14.250000\n",
      "66 Episode finished after 42 time steps / mean 14.730000\n",
      "67 Episode finished after 15 time steps / mean 15.150000\n",
      "68 Episode finished after 55 time steps / mean 15.300000\n",
      "69 Episode finished after 28 time steps / mean 15.850000\n",
      "70 Episode finished after 18 time steps / mean 16.130000\n",
      "71 Episode finished after 32 time steps / mean 16.310000\n",
      "72 Episode finished after 24 time steps / mean 16.630000\n",
      "73 Episode finished after 34 time steps / mean 16.870000\n",
      "74 Episode finished after 38 time steps / mean 17.210000\n",
      "75 Episode finished after 20 time steps / mean 17.590000\n",
      "76 Episode finished after 13 time steps / mean 17.790000\n",
      "77 Episode finished after 30 time steps / mean 17.920000\n",
      "78 Episode finished after 14 time steps / mean 18.220000\n",
      "79 Episode finished after 28 time steps / mean 18.360000\n",
      "80 Episode finished after 12 time steps / mean 18.640000\n",
      "81 Episode finished after 15 time steps / mean 18.760000\n",
      "82 Episode finished after 15 time steps / mean 18.910000\n",
      "83 Episode finished after 34 time steps / mean 19.060000\n",
      "84 Episode finished after 30 time steps / mean 19.400000\n",
      "85 Episode finished after 48 time steps / mean 19.700000\n",
      "86 Episode finished after 12 time steps / mean 20.180000\n",
      "87 Episode finished after 15 time steps / mean 20.300000\n",
      "88 Episode finished after 52 time steps / mean 20.450000\n",
      "89 Episode finished after 32 time steps / mean 20.970000\n",
      "90 Episode finished after 15 time steps / mean 21.290000\n",
      "91 Episode finished after 22 time steps / mean 21.440000\n",
      "92 Episode finished after 25 time steps / mean 21.660000\n",
      "93 Episode finished after 29 time steps / mean 21.910000\n",
      "94 Episode finished after 31 time steps / mean 22.200000\n",
      "95 Episode finished after 23 time steps / mean 22.510000\n",
      "96 Episode finished after 41 time steps / mean 22.740000\n",
      "97 Episode finished after 29 time steps / mean 23.150000\n",
      "98 Episode finished after 26 time steps / mean 23.440000\n",
      "99 Episode finished after 24 time steps / mean 23.700000\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def iterate_value_function(v_inp, gamma, env):\n",
    "    ret = np.zeros(env.nS)\n",
    "    for sid in range(env.nS):\n",
    "        temp_v = np.zeros(env.nA)\n",
    "        for action in range(env.nA):\n",
    "            for (prob, dst_state, reward, is_final) in env.P[sid][action]:\n",
    "                temp_v[action] += prob*(reward + gamma*v_inp[dst_state]*(not is_final))\n",
    "        ret[sid] = max(temp_v)\n",
    "    return ret\n",
    "\n",
    "def build_greedy_policy(v_inp, gamma, env):\n",
    "    new_policy = np.zeros(env.nS)\n",
    "    for state_id in range(env.nS):\n",
    "        profits = np.zeros(env.nA)\n",
    "        for action in range(env.nA):\n",
    "            for (prob, dst_state, reward, is_final) in env.P[state_id][action]:\n",
    "                profits[action] += prob*(reward + gamma*v_inp[dst_state])  # v[dst_state] は v_inp のタイポ?\n",
    "        new_policy[state_id] = np.argmax(profits)\n",
    "    return new_policy\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "goal_average_steps = 195\n",
    "max_number_of_steps = 500\n",
    "num_consecutive_iterations = 100\n",
    "num_episodes = 100\n",
    "last_time_steps = np.zeros(num_consecutive_iterations)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # 環境の初期化\n",
    "    observation = env.reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "    for t in range(max_number_of_steps):\n",
    "        # CartPoleの描画\n",
    "        env.render()\n",
    "    \n",
    "        # ランダムで行動の選択\n",
    "        action = np.random.choice([0, 1])\n",
    "\n",
    "        # 行動の実行とフィードバックの取得\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print('%d Episode finished after %d time steps / mean %f' % (episode, t + 1,\n",
    "                last_time_steps.mean()))\n",
    "            last_time_steps = np.hstack((last_time_steps[1:], [episode_reward]))\n",
    "            break\n",
    "\n",
    "    if (last_time_steps.mean() >= goal_average_steps): # 直近の100エピソードが195以上であれば成功\n",
    "        print('Episode %d train agent successfuly!' % episode)\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "beb4cf9dae92875600228d3972fe0816dec86166e2e7bb5e634c22474cb517ea"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
